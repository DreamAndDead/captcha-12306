# how it works

## dataset

数据集是工程的第一步。


### raw data

在开始项目之前，使用脚本在 12306 网站爬取了 1w 张验证码图片，日期 2020.02.14。

![captcha example](./example.jpg)

每张验证码中包含 **1** 个文本(text)，**8** 个图像(image)。

比如上面的验证码，文本内容为 `航母`，图像标签依次为 `冰箱 航母 黑板 剪纸 红枣 调色板 航母 红枣`。

所以 1w 张验证码包含
- 1w 个文本(text)
- 8w 个图像(image)

### crop

下载得到的验证码都是 raw 数据，我们要将其分解为单独的部分，文本(text)图片放在一起，图像(image)图片放在一起。

#### text

text 所占的区域起始位置是固定的，但是终止位置不确定。

![text crop area](./crop.jpg)

因为 text 用灰/黑色标识，背景是白色，多余的空间就可以进一步分析从而去除，使截取得到的 text 图片尽量只包含文字。

![text crop bounding area](./crop-bounding.jpg)

这样得到的 text 图片没有统一的 `shape`，无论是高度还是宽度，后面会看到，这一点并未过于影响精度。

#### image

相比之下，每个 image 的起始位置和大小都是固定的，只需要固定截取即可。

### label

上面已经将 text image 分开，但是还要对数据进行**标注**，才可以提供给网络进行训练。

#### text

首先来标注 text 图片，因为相对 image 图片而言比较简单，形状比较小（平均大小约 50x30 px），纯黑白色彩。

即使如此，完全从人工开始还是费力的，毕竟 1w 条 text 数据，
所以一开始借助 `baidu ocr api` 进行初步的识别，后续再手动校正。

将全部 1w 条数据使用 ocr 识别之后，对所有结果做个数统计，从降序排列，

```

```

我们可以观察到，有明确意义的标注大约有 80+ 个（准确来说应该是 80 个），如日历，菠萝，
无意义的标注种类更多，但是每个标注对应的个数比较小，如E历，波萝。

统计纠正所有标注结果后，有理由相信，验证码 text 一共只有 80 个分类。

```

```

所有这 80 个分类外的结果都是错误的，如此来看，ocr 分类的正确率大概 60%（6000+ 张识别正确）。
原因可能是因为 ocr 接口是一个通用接口，它先将图片中切块分出单字，再对单字进行识别，最后组成单字识别的结果为总结果。

由于我们知道了所有数据的分类数量，在这样一个限定的范围内训练网络进行分类，自然有理由相信要比 ocr 接口做的更好。

对于剩下的 4000 张图，完全进行人工分类，也不是简单的工作。

一个思路，可以使用 kmeans 算法将 4000 张图片进行自聚类为 80 个类，虽然聚类并不一定完美，但是每个类中包含了多数相似的图片，这时再进行人工标注可能会简单一些。

另一个思路，可以使用图像相似性算法，将未分类的图片同已经分类的图片计算相似度，用最相似的图片的标注来作为未得到标注的图片标注。

尽管如此，上面的方法并非完全正确，总有部分数据需要手动来标识。


#### image

第一反映就是，同样的思路，先使用 baidu 识图接口进行一个粗略的分类，剩下的聚类后再进行手动处理。

然而同样的方法对于图片来讲不太可行。
首先，识图接口有数量限制，500次每天；
而且，由于识图接口也是一个通用接口，会得到五花八门的结果，不一定落在 80 个类中，
并且同一个物体，可能有不同层次的描述，比如 `动物->老虎->华南虎`。

所以最终得到的分类结果难以整理。

这里图片分类有一个前提，12306 没有使用 80 个类之外的图片来混淆（事实上的确没有）。
如果还有很多 80 分类外的图片，无论是分类还是训练都会更加困难，但是目前好像没有这样做，image 图片都可以对应上这 80 个分类。


在[另一个项目](easy12306)中，得到一个非常不错的想法，其中提到用统计学的方法来分类图片。

[easy12306]: http://jakdfjakfd;aj

存在于一张验证码中的 8 张 image 图片，是 text 分类的可能性为 1/8，而是其它分类的可能性为 1/80。
我们可以大胆的将这 8 张图片进行标注为 text 表示的分类（在 text 分类正确率非常高的情况下）。

最终标注后的 image 数据，每个分类中大约只有 1/8 的图片是正确的标注，其它都是错误标注。
但是平均情况下，每张验证码中会有 2 个 image 表示 text 分类，所以每个分类中大约有 1/4 是分类正确的。

如果用这个数据集来训练网络，进而来识别分类 image 图片，因为分类结果选择最大概率的结果，
即使数据集标注并非完全正确，依然可以得到一个不错的结果。

这一点可以这样理解，如果标注完全正确，正确概率和随机概率是 1/1 和 1/80 的差距；
目前的标注方法，则是 1/4 和 1/80 的差距。

当训练得到的网络进行 predict 的时候，1/4 概率高于 1/80，又因为偏向使用最大概率结果，所以这样标注的数据集理论上依然会有不错的分类正确率。

[提到这个方法的项目][easy12306]在用这样的数据集训练时，使用外部提供的正确标注数据集来表示 `val_acc`，这样可以得到准确的正确率。

如果在当前标注的数据集上分出 test 部分进行验证，理论上，`val_acc` 越接近 25% 效果越好。


在得到这样的网络后，对所有 image 图片进行分类，其中肯定有分类错误的部分，再人工审核一下，得到完全正确标注的数据集，用于之后的训练。




实际上这种办法并不可行，难道必须要用正确的验证集才可以吗？
与其说是学习，不如说是记忆。

越复杂的网络，对已见过的东西记忆力越好。
如果训练数据的分类错误，再见到一个很相似的图片，那么识别出的分类一定也是错误的。




### train

在所有数据都得到正确标注的前提下，训练模型。

#### text

对于所有 1w 张 text 图片，如果 12306 每次提供的验证码是随机的，一共 80 个类，每个类平均约 125 张。
验证分类后的数据，大致就是平均的分布。

每个类 125 张，分去 test 部分的数据，用于 train 的可能 100 张，感觉量不够充足。

但是 text 图片确实比较简单，颜色和大小都有限，并且变形幅度不大，使用 `LeNet` 在这个数据集上训练，模型 `val_acc` 可达到95%。

#### image

不同于 text 图片，image 图片共 8w 张，一共 80 个类，每个类约 1000 张图片，数据量还算可以。

image 训练使用 VGG16 在 imagenet 训练好的模型，使用 fine tuning 来训练。



